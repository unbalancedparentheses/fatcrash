{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 09 — Methods Comparison (All 13 Estimators)\n",
    "\n",
    "This notebook compares **all 13 fat-tail estimators** in the fatcrash library across **4 assets**.\n",
    "\n",
    "**Estimators covered:**\n",
    "\n",
    "| # | Method | Category |\n",
    "|---|--------|----------|\n",
    "| 1 | Hill estimator | Tail index |\n",
    "| 2 | DEH (Dekkers-Einmahl-de Haan) | Tail index |\n",
    "| 3 | QQ estimator | Tail index |\n",
    "| 4 | Pickands estimator | Tail index |\n",
    "| 5 | Max-stability kappa | Kappa metric |\n",
    "| 6 | Taleb kappa | Kappa metric |\n",
    "| 7 | Max-to-Sum ratio | Concentration |\n",
    "| 8 | GPD fit | Extreme Value Theory |\n",
    "| 9 | GPD VaR/ES | Extreme Value Theory |\n",
    "| 10 | GEV fit | Extreme Value Theory |\n",
    "| 11 | Hurst exponent | Persistence / memory |\n",
    "| 12 | DFA exponent | Persistence / memory |\n",
    "| 13 | Spectral exponent | Persistence / memory |\n",
    "\n",
    "**Assets:** BTC, SPY, Gold, GBP/USD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\nfrom fatcrash.data.ingest import from_sample, from_csv, load_fred_forex\nfrom fatcrash.data.transforms import log_returns, negative_returns, block_maxima\nfrom fatcrash._core import (\n    hill_estimator, hill_rolling,\n    kappa_metric, kappa_rolling,\n    taleb_kappa, taleb_kappa_rolling,\n    pickands_estimator, pickands_rolling,\n    hurst_exponent, hurst_rolling,\n    dfa_exponent, dfa_rolling,\n    deh_estimator, deh_rolling,\n    qq_estimator, qq_rolling,\n    maxsum_ratio, maxsum_rolling,\n    spectral_exponent, spectral_rolling,\n    gpd_fit, gpd_var_es, gev_fit,\n    lppls_fit,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 4 assets\n",
    "btc = from_sample('btc')\n",
    "spy = from_sample('spy')\n",
    "gold = from_sample('gold')\n",
    "gbpusd = from_csv('data/sample/gbpusd_daily.csv')\n",
    "\n",
    "assets = {'BTC': btc, 'SPY': spy, 'Gold': gold, 'GBP/USD': gbpusd}\n",
    "\n",
    "# Compute returns for each asset\n",
    "returns = {}\n",
    "for name, df in assets.items():\n",
    "    returns[name] = log_returns(df)\n",
    "\n",
    "for name, df in assets.items():\n",
    "    ret = returns[name]\n",
    "    print(f\"{name}: {len(df)} days, {len(ret)} returns, \"\n",
    "          f\"{df.index[0].date()} to {df.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Tail Index Comparison (Hill, DEH, QQ)\n",
    "\n",
    "Three estimators of the tail index alpha. Lower alpha means fatter tails.\n",
    "- **alpha < 2**: infinite variance (extremely fat)\n",
    "- **alpha < 4**: fat tails, finite variance but infinite kurtosis\n",
    "- **alpha > 4**: approaching thin-tailed behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_results = []\n",
    "for name in assets:\n",
    "    ret = returns[name]\n",
    "    hill_a = hill_estimator(ret)\n",
    "    deh_a = deh_estimator(ret)\n",
    "    qq_a = qq_estimator(ret)\n",
    "    tail_results.append({\n",
    "        'Asset': name,\n",
    "        'Hill alpha': round(hill_a, 3),\n",
    "        'DEH alpha': round(deh_a, 3),\n",
    "        'QQ alpha': round(qq_a, 3),\n",
    "        'N': len(ret),\n",
    "    })\n",
    "\n",
    "tail_df = pd.DataFrame(tail_results)\n",
    "print(tail_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(tail_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, tail_df['Hill alpha'], width, label='Hill', color='#2196F3')\n",
    "ax.bar(x, tail_df['DEH alpha'], width, label='DEH', color='#FF9800')\n",
    "ax.bar(x + width, tail_df['QQ alpha'], width, label='QQ', color='#4CAF50')\n",
    "\n",
    "ax.axhline(y=2, color='red', linestyle='--', linewidth=1, label='alpha=2 (infinite variance)')\n",
    "ax.axhline(y=4, color='green', linestyle='--', linewidth=1, label='alpha=4 (thin tail boundary)')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tail_df['Asset'])\n",
    "ax.set_ylabel('Tail index alpha (lower = fatter tails)')\n",
    "ax.set_title('Tail Index Comparison: Hill vs DEH vs QQ')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Kappa Metrics (Max-stability Kappa + Taleb Kappa)\n",
    "\n",
    "- **Max-stability kappa**: kappa below the Gaussian benchmark signals fat tails.\n",
    "- **Taleb kappa**: kappa above the benchmark signals fat tails (inverse convention).\n",
    "\n",
    "Both measure departure from Gaussian max-stability, but with opposite sign conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_results = []\n",
    "for name in assets:\n",
    "    ret = returns[name]\n",
    "    k_val, k_bench = kappa_metric(ret, n_subsamples=10)\n",
    "    tk_val, tk_bench = taleb_kappa(ret)\n",
    "    kappa_results.append({\n",
    "        'Asset': name,\n",
    "        'Kappa': round(k_val, 4),\n",
    "        'Kappa Bench': round(k_bench, 4),\n",
    "        'Kappa < Bench?': 'FAT' if k_val < k_bench else 'thin',\n",
    "        'Taleb Kappa': round(tk_val, 4),\n",
    "        'Taleb Bench': round(tk_bench, 4),\n",
    "        'Taleb > Bench?': 'FAT' if tk_val > tk_bench else 'thin',\n",
    "    })\n",
    "\n",
    "kappa_df = pd.DataFrame(kappa_results)\n",
    "print(kappa_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = np.arange(len(kappa_df))\n",
    "width = 0.35\n",
    "\n",
    "# Max-stability kappa\n",
    "ax1.bar(x - width/2, kappa_df['Kappa'], width, label='Kappa', color='#2196F3')\n",
    "ax1.bar(x + width/2, kappa_df['Kappa Bench'], width, label='Gaussian Benchmark', color='#9E9E9E')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(kappa_df['Asset'])\n",
    "ax1.set_ylabel('Kappa')\n",
    "ax1.set_title('Max-stability Kappa (below bench = fat tails)')\n",
    "ax1.legend()\n",
    "\n",
    "# Taleb kappa\n",
    "ax2.bar(x - width/2, kappa_df['Taleb Kappa'], width, label='Taleb Kappa', color='#FF5722')\n",
    "ax2.bar(x + width/2, kappa_df['Taleb Bench'], width, label='Gaussian Benchmark', color='#9E9E9E')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(kappa_df['Asset'])\n",
    "ax2.set_ylabel('Taleb Kappa')\n",
    "ax2.set_title('Taleb Kappa (above bench = fat tails)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Max-to-Sum Ratio\n",
    "\n",
    "The max-to-sum ratio measures how much a single observation dominates the total.\n",
    "- For thin-tailed data, the ratio converges to 0 as N grows.\n",
    "- For fat-tailed data (alpha < 2), the ratio stays bounded away from 0.\n",
    "- Higher ratios indicate heavier concentration of risk in single events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsum_results = []\n",
    "for name in assets:\n",
    "    ret = returns[name]\n",
    "    ratio = maxsum_ratio(ret)\n",
    "    maxsum_results.append({'Asset': name, 'Max-to-Sum Ratio': round(ratio, 5)})\n",
    "\n",
    "maxsum_df = pd.DataFrame(maxsum_results)\n",
    "print(maxsum_df.to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = ['#E53935' if r > 0.05 else '#43A047' for r in maxsum_df['Max-to-Sum Ratio']]\n",
    "ax.bar(maxsum_df['Asset'], maxsum_df['Max-to-Sum Ratio'], color=colors)\n",
    "ax.axhline(y=0.05, color='orange', linestyle='--', linewidth=1, label='0.05 threshold')\n",
    "ax.set_ylabel('Max-to-Sum Ratio')\n",
    "ax.set_title('Max-to-Sum Ratio by Asset')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation: A higher ratio means a single extreme event accounts for\")\n",
    "print(\"a larger fraction of the total sum of absolute returns. Values above ~0.05\")\n",
    "print(\"suggest meaningful tail concentration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. EVT: VaR, Expected Shortfall, and GEV Shape\n",
    "\n",
    "**GPD** (Generalized Pareto Distribution) models exceedances over a threshold to estimate:\n",
    "- **VaR**: Value-at-Risk at the 99% confidence level\n",
    "- **ES**: Expected Shortfall (average loss beyond VaR)\n",
    "\n",
    "**GEV** (Generalized Extreme Value) fits block maxima:\n",
    "- **xi > 0**: Frechet (fat tails)\n",
    "- **xi ~ 0**: Gumbel (exponential tails)\n",
    "- **xi < 0**: Weibull (bounded tails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_results = []\n",
    "for name in assets:\n",
    "    ret = returns[name]\n",
    "    try:\n",
    "        sigma, xi, threshold, n_exc = gpd_fit(ret, quantile=0.95)\n",
    "        var99, es99 = gpd_var_es(ret, p=0.99, quantile=0.95)\n",
    "        var95, es95 = gpd_var_es(ret, p=0.95, quantile=0.95)\n",
    "        evt_results.append({\n",
    "            'Asset': name,\n",
    "            'GPD sigma': round(sigma, 5),\n",
    "            'GPD xi': round(xi, 3),\n",
    "            'VaR 95%': round(var95, 4),\n",
    "            'VaR 99%': round(var99, 4),\n",
    "            'ES 99%': round(es99, 4),\n",
    "            'Exceedances': n_exc,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        evt_results.append({'Asset': name, 'Error': str(e)})\n",
    "\n",
    "evt_df = pd.DataFrame(evt_results)\n",
    "print(\"GPD Results:\")\n",
    "print(evt_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VaR and ES bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(evt_df))\n",
    "width = 0.3\n",
    "\n",
    "ax.bar(x - width, evt_df['VaR 95%'], width, label='VaR 95%', color='#FFC107')\n",
    "ax.bar(x, evt_df['VaR 99%'], width, label='VaR 99%', color='#FF5722')\n",
    "ax.bar(x + width, evt_df['ES 99%'], width, label='ES 99%', color='#B71C1C')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(evt_df['Asset'])\n",
    "ax.set_ylabel('Loss (log returns)')\n",
    "ax.set_title('Tail Risk: GPD-based VaR and Expected Shortfall')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEV shape parameter\n",
    "gev_results = []\n",
    "for name in assets:\n",
    "    ret = returns[name]\n",
    "    bm = block_maxima(ret, block_size=21)\n",
    "    mu, sigma, xi = gev_fit(bm)\n",
    "    tail_type = 'Frechet (fat)' if xi > 0.05 else ('Gumbel' if xi > -0.05 else 'Weibull (bounded)')\n",
    "    gev_results.append({\n",
    "        'Asset': name,\n",
    "        'GEV mu': round(mu, 5),\n",
    "        'GEV sigma': round(sigma, 5),\n",
    "        'GEV xi': round(xi, 3),\n",
    "        'Tail type': tail_type,\n",
    "    })\n",
    "\n",
    "gev_df = pd.DataFrame(gev_results)\n",
    "print(\"GEV Block Maxima Results (block_size=21):\")\n",
    "print(gev_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Persistence: Hurst, DFA, Spectral\n",
    "\n",
    "Three measures of long-range dependence / persistence:\n",
    "- **Hurst exponent**: H = 0.5 (random walk), H > 0.5 (persistent), H < 0.5 (mean-reverting)\n",
    "- **DFA exponent**: similar interpretation to Hurst, estimated via detrended fluctuation analysis\n",
    "- **Spectral exponent**: estimated from the power spectrum; higher values indicate more persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_results = []\n",
    "for name in assets:\n",
    "    ret = returns[name]\n",
    "    h = hurst_exponent(ret)\n",
    "    d = dfa_exponent(ret)\n",
    "    s = spectral_exponent(ret)\n",
    "    persist_results.append({\n",
    "        'Asset': name,\n",
    "        'Hurst H': round(h, 4),\n",
    "        'DFA alpha': round(d, 4),\n",
    "        'Spectral beta': round(s, 4),\n",
    "    })\n",
    "\n",
    "persist_df = pd.DataFrame(persist_results)\n",
    "print(persist_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(persist_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, persist_df['Hurst H'], width, label='Hurst H', color='#673AB7')\n",
    "ax.bar(x, persist_df['DFA alpha'], width, label='DFA alpha', color='#00BCD4')\n",
    "ax.bar(x + width, persist_df['Spectral beta'], width, label='Spectral beta', color='#8BC34A')\n",
    "\n",
    "ax.axhline(y=0.5, color='grey', linestyle='--', linewidth=1, label='H=0.5 (random walk)')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(persist_df['Asset'])\n",
    "ax.set_ylabel('Exponent value')\n",
    "ax.set_title('Persistence Measures: Hurst, DFA, Spectral')\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Rolling Indicators: Hill Alpha Across Assets\n",
    "\n",
    "252-day rolling window of Hill tail index alpha for all 4 assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharex=False, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, df) in enumerate(assets.items()):\n",
    "    ret = returns[name]\n",
    "    alpha_rolling = np.asarray(hill_rolling(ret, window=252))\n",
    "    dates = df.index[1:]  # returns are 1 shorter than the DataFrame\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.plot(dates, alpha_rolling, linewidth=0.8, color='#1565C0')\n",
    "    ax.axhline(y=2, color='red', linestyle='--', linewidth=0.8, label='alpha=2')\n",
    "    ax.axhline(y=4, color='green', linestyle='--', linewidth=0.8, label='alpha=4')\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylabel('Hill alpha')\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "fig.suptitle('Rolling Hill Alpha (252-day window)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. 2017 BTC Bubble Deep Dive\n",
    "\n",
    "Zoom into the 2017 BTC bubble (2017-01-01 to 2018-03-01) and overlay multiple rolling indicators\n",
    "to see how they behave before, during, and after the crash.\n",
    "\n",
    "Vertical red line marks the peak at **2017-12-17**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_2017 = btc.loc['2017-01-01':'2018-03-01']\n",
    "ret_2017 = log_returns(btc_2017)\n",
    "dates_2017 = btc_2017.index[1:]  # match returns length\n",
    "\n",
    "# Rolling indicators (60-day window)\n",
    "hill_60 = np.asarray(hill_rolling(ret_2017, window=60))\n",
    "dfa_60 = np.asarray(dfa_rolling(ret_2017, window=60))\n",
    "kappa_60, kappa_bench_60 = kappa_rolling(ret_2017, window=60, n_subsamples=5)\n",
    "kappa_60 = np.asarray(kappa_60)\n",
    "\n",
    "peak_date = pd.Timestamp('2017-12-17')\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Subplot 1: Price\n",
    "axes[0].plot(btc_2017.index, btc_2017['close'], color='#FF9800', linewidth=1.2)\n",
    "axes[0].set_ylabel('BTC Price (USD)')\n",
    "axes[0].set_title('BTC Price')\n",
    "axes[0].axvline(x=peak_date, color='red', linestyle='--', linewidth=1, label='Peak (2017-12-17)')\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# Subplot 2: Rolling Hill alpha\n",
    "axes[1].plot(dates_2017, hill_60, color='#1565C0', linewidth=1)\n",
    "axes[1].axhline(y=2, color='red', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "axes[1].axhline(y=4, color='green', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "axes[1].set_ylabel('Hill alpha')\n",
    "axes[1].set_title('Rolling Hill Alpha (60d)')\n",
    "axes[1].axvline(x=peak_date, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# Subplot 3: Rolling DFA\n",
    "axes[2].plot(dates_2017, dfa_60, color='#00BCD4', linewidth=1)\n",
    "axes[2].axhline(y=0.5, color='grey', linestyle='--', linewidth=0.7, alpha=0.7, label='H=0.5')\n",
    "axes[2].set_ylabel('DFA exponent')\n",
    "axes[2].set_title('Rolling DFA Exponent (60d)')\n",
    "axes[2].axvline(x=peak_date, color='red', linestyle='--', linewidth=1)\n",
    "axes[2].legend(loc='upper left')\n",
    "\n",
    "# Subplot 4: Rolling Kappa\n",
    "axes[3].plot(dates_2017, kappa_60, color='#E91E63', linewidth=1, label='Kappa')\n",
    "axes[3].axhline(y=kappa_bench_60, color='grey', linestyle='--', linewidth=0.7, label=f'Gaussian bench ({kappa_bench_60:.3f})')\n",
    "axes[3].set_ylabel('Kappa')\n",
    "axes[3].set_title('Rolling Max-stability Kappa (60d)')\n",
    "axes[3].axvline(x=peak_date, color='red', linestyle='--', linewidth=1)\n",
    "axes[3].legend(loc='upper left')\n",
    "\n",
    "axes[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Method Agreement Matrix\n",
    "\n",
    "For each asset, classify each method as detecting fat tails (FAT) or not (thin).\n",
    "\n",
    "**Thresholds used:**\n",
    "- Hill, DEH, QQ: alpha < 4\n",
    "- Max-stability Kappa: kappa < benchmark\n",
    "- Taleb Kappa: kappa > benchmark\n",
    "- Pickands: gamma > 0 (i.e., xi = 1/gamma implies fat tail)\n",
    "- Max-to-Sum: ratio > 0.05\n",
    "- GPD: xi > 0\n",
    "- GEV: xi > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement = []\n",
    "for name in assets:\n",
    "    ret = returns[name]\n",
    "\n",
    "    # Tail index estimators\n",
    "    hill_a = hill_estimator(ret)\n",
    "    deh_a = deh_estimator(ret)\n",
    "    qq_a = qq_estimator(ret)\n",
    "    pick_g = pickands_estimator(ret)\n",
    "\n",
    "    # Kappa metrics\n",
    "    k_val, k_bench = kappa_metric(ret, n_subsamples=10)\n",
    "    tk_val, tk_bench = taleb_kappa(ret)\n",
    "\n",
    "    # Max-to-Sum\n",
    "    ms = maxsum_ratio(ret)\n",
    "\n",
    "    # EVT\n",
    "    try:\n",
    "        _, gpd_xi, _, _ = gpd_fit(ret, quantile=0.95)\n",
    "        gpd_fat = gpd_xi > 0\n",
    "    except Exception:\n",
    "        gpd_xi = float('nan')\n",
    "        gpd_fat = None\n",
    "\n",
    "    bm = block_maxima(ret, block_size=21)\n",
    "    _, _, gev_xi = gev_fit(bm)\n",
    "\n",
    "    row = {\n",
    "        'Asset': name,\n",
    "        'Hill (a<4)': 'FAT' if hill_a < 4 else 'thin',\n",
    "        'DEH (a<4)': 'FAT' if deh_a < 4 else 'thin',\n",
    "        'QQ (a<4)': 'FAT' if qq_a < 4 else 'thin',\n",
    "        'Pickands (g>0)': 'FAT' if pick_g > 0 else 'thin',\n",
    "        'Kappa (<bench)': 'FAT' if k_val < k_bench else 'thin',\n",
    "        'Taleb (>bench)': 'FAT' if tk_val > tk_bench else 'thin',\n",
    "        'MaxSum (>0.05)': 'FAT' if ms > 0.05 else 'thin',\n",
    "        'GPD (xi>0)': 'FAT' if gpd_fat else ('thin' if gpd_fat is not None else 'N/A'),\n",
    "        'GEV (xi>0)': 'FAT' if gev_xi > 0 else 'thin',\n",
    "    }\n",
    "\n",
    "    # Count agreement\n",
    "    fat_count = sum(1 for k, v in row.items() if k != 'Asset' and v == 'FAT')\n",
    "    total = sum(1 for k, v in row.items() if k != 'Asset' and v != 'N/A')\n",
    "    row['Agreement'] = f'{fat_count}/{total}'\n",
    "    agreement.append(row)\n",
    "\n",
    "agree_df = pd.DataFrame(agreement)\n",
    "print(agree_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 10. Edge Cases: Known Distributions\n",
    "\n",
    "Run all estimators on synthetic data with known properties to verify they produce\n",
    "sensible results:\n",
    "- **Gaussian**: alpha = infinity (no fat tails)\n",
    "- **Student-t(3)**: alpha = 3 (fat tails, finite variance, infinite kurtosis)\n",
    "- **Cauchy**: alpha = 1 (extremely fat tails, infinite variance)\n",
    "- **Pareto(2)**: alpha = 2 (fat tails, infinite variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "n = 5000\n",
    "\n",
    "distributions = {\n",
    "    'Gaussian': rng.standard_normal(n),\n",
    "    'Student-t(3)': rng.standard_t(3, n),\n",
    "    'Cauchy': rng.standard_cauchy(n),\n",
    "    'Pareto(2)': rng.pareto(2, n),\n",
    "}\n",
    "\n",
    "edge_results = []\n",
    "for dist_name, data in distributions.items():\n",
    "    # Tail index estimators\n",
    "    hill_a = hill_estimator(data)\n",
    "    deh_a = deh_estimator(data)\n",
    "    qq_a = qq_estimator(data)\n",
    "    pick_g = pickands_estimator(data)\n",
    "\n",
    "    # Kappa metrics\n",
    "    k_val, k_bench = kappa_metric(data, n_subsamples=10)\n",
    "    tk_val, tk_bench = taleb_kappa(data)\n",
    "\n",
    "    # Max-to-Sum\n",
    "    ms = maxsum_ratio(data)\n",
    "\n",
    "    # EVT\n",
    "    try:\n",
    "        _, gpd_xi, _, _ = gpd_fit(data, quantile=0.95)\n",
    "    except Exception:\n",
    "        gpd_xi = float('nan')\n",
    "\n",
    "    try:\n",
    "        bm = block_maxima(data, block_size=21)\n",
    "        _, _, gev_xi = gev_fit(bm)\n",
    "    except Exception:\n",
    "        gev_xi = float('nan')\n",
    "\n",
    "    # Persistence\n",
    "    h = hurst_exponent(data)\n",
    "    d = dfa_exponent(data)\n",
    "    s = spectral_exponent(data)\n",
    "\n",
    "    edge_results.append({\n",
    "        'Distribution': dist_name,\n",
    "        'Hill alpha': round(hill_a, 2),\n",
    "        'DEH alpha': round(deh_a, 2),\n",
    "        'QQ alpha': round(qq_a, 2),\n",
    "        'Pickands gamma': round(pick_g, 3),\n",
    "        'Kappa': round(k_val, 4),\n",
    "        'K Bench': round(k_bench, 4),\n",
    "        'Taleb K': round(tk_val, 4),\n",
    "        'TK Bench': round(tk_bench, 4),\n",
    "        'MaxSum': round(ms, 4),\n",
    "        'GPD xi': round(gpd_xi, 3),\n",
    "        'GEV xi': round(gev_xi, 3),\n",
    "        'Hurst': round(h, 3),\n",
    "        'DFA': round(d, 3),\n",
    "        'Spectral': round(s, 3),\n",
    "    })\n",
    "\n",
    "edge_df = pd.DataFrame(edge_results)\n",
    "print(edge_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ryih289tzsm",
   "source": "## 10b. FRED Forex: All 23 Currency Pairs\n\nThe four sample assets and synthetic distributions above test our methods on a handful\nof series. But to establish that **fat tails are universal in exchange rates**, we need\na broader sample.\n\nWe load **23 daily currency pairs** from FRED via the\n[forex-centuries](https://github.com/unbalancedparentheses/forex-centuries) repository\nand run every estimator on each pair. This is the same dataset used in the\n[accuracy report](../analysis/accuracy_report.py).\n\n**Data requirements:**\n```bash\ngit clone https://github.com/unbalancedparentheses/forex-centuries ~/projects/forex-centuries\n```\nOr set `FOREX_CENTURIES_DIR` to your clone location.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uobhua8kh8t",
   "source": "# Load all 23 FRED forex pairs\nfred_pairs = load_fred_forex()\nprint(f\"Loaded {len(fred_pairs)} FRED currency pairs\\n\")\n\n# Run all estimators (tail, kappa, EVT, persistence) on each pair\nfred_rows = []\nfor pair_name, pair_df in sorted(fred_pairs.items()):\n    r = log_returns(pair_df)\n    l = negative_returns(r)\n\n    if len(l) < 100:\n        print(f\"  Skipping {pair_name}: only {len(l)} observations\")\n        continue\n\n    # Tail index estimators\n    hill_a = hill_estimator(l)\n    deh_g = deh_estimator(l)\n    qq_a = qq_estimator(l)\n    pick_g = pickands_estimator(l)\n\n    # Kappa metrics\n    k_val, k_bench = kappa_metric(r, n_subsamples=10)\n    tk_val, tk_bench = taleb_kappa(r)\n\n    # Max-to-Sum\n    ms = maxsum_ratio(l)\n\n    # EVT\n    try:\n        sigma, gpd_xi, _, n_exc = gpd_fit(r, quantile=0.95)\n        var99, es99 = gpd_var_es(r, p=0.99, quantile=0.95)\n    except Exception:\n        gpd_xi, var99, es99 = float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    try:\n        bm = block_maxima(r, block_size=21)\n        _, _, gev_xi = gev_fit(bm)\n    except Exception:\n        gev_xi = float(\"nan\")\n\n    # Persistence\n    h = hurst_exponent(r)\n    d = dfa_exponent(r)\n    s = spectral_exponent(r)\n\n    fred_rows.append({\n        \"Pair\": pair_name,\n        \"N\": len(r),\n        \"Hill alpha\": hill_a,\n        \"QQ alpha\": qq_a,\n        \"DEH gamma\": deh_g,\n        \"Pickands gamma\": pick_g,\n        \"Kappa\": k_val,\n        \"Taleb K\": tk_val,\n        \"MaxSum\": ms,\n        \"GPD xi\": gpd_xi,\n        \"GEV xi\": gev_xi,\n        \"VaR 99%\": var99,\n        \"ES 99%\": es99,\n        \"Hurst H\": h,\n        \"DFA alpha\": d,\n        \"Spectral d\": s,\n    })\n\nfred_all = pd.DataFrame(fred_rows).set_index(\"Pair\")\n\nprint(f\"FRED Forex: All {len(fred_all)} Pairs — Complete Estimator Comparison\")\nprint(\"=\" * 100)\ndisplay(fred_all.style.format(\n    {col: \"{:.4f}\" for col in fred_all.columns if col != \"N\"}\n).format({\"N\": \"{:,d}\"}))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dwf68nge6c",
   "source": "# Method agreement matrix for all 23 FRED forex pairs\nfred_agree = []\nfor pair_name in fred_all.index:\n    row = fred_all.loc[pair_name]\n\n    checks = {\n        \"Hill (a<4)\":     row[\"Hill alpha\"] < 4,\n        \"QQ (a<4)\":       row[\"QQ alpha\"] < 4,\n        \"DEH (g>0)\":      row[\"DEH gamma\"] > 0,\n        \"Pickands (g>0)\": row[\"Pickands gamma\"] > 0,\n        \"GPD (xi>0)\":     row[\"GPD xi\"] > 0 if not np.isnan(row[\"GPD xi\"]) else None,\n        \"GEV (xi>0)\":     row[\"GEV xi\"] > 0 if not np.isnan(row[\"GEV xi\"]) else None,\n        \"Hurst (>0.5)\":   row[\"Hurst H\"] > 0.5,\n        \"DFA (>0.5)\":     row[\"DFA alpha\"] > 0.5,\n    }\n\n    fat_count = sum(1 for v in checks.values() if v is True)\n    total = sum(1 for v in checks.values() if v is not None)\n\n    fred_agree.append({\n        \"Pair\": pair_name,\n        **{k: (\"FAT\" if v else \"thin\") if v is not None else \"N/A\" for k, v in checks.items()},\n        \"Agreement\": f\"{fat_count}/{total}\",\n    })\n\nfred_agree_df = pd.DataFrame(fred_agree).set_index(\"Pair\")\n\nprint(\"Method Agreement: FRED Forex Pairs\")\nprint(\"=\" * 100)\nprint(f\"(FAT = method detects fat tails / persistence)\\n\")\ndisplay(fred_agree_df)\n\n# Count how many pairs each method flags\nprint(\"\\nMethod detection rates:\")\nfor col in fred_agree_df.columns:\n    if col == \"Agreement\":\n        continue\n    fat_count = (fred_agree_df[col] == \"FAT\").sum()\n    total = (fred_agree_df[col] != \"N/A\").sum()\n    print(f\"  {col:20s}: {fat_count}/{total} pairs ({fat_count/total*100:.0f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "kqkff9zxi7m",
   "source": "# Visualizations for FRED forex results\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Panel 1: Hill alpha sorted bar chart\nsorted_hill = fred_all[\"Hill alpha\"].sort_values()\ncolors_hill = [\"#E53935\" if a < 2 else \"#FF9800\" if a < 4 else \"#43A047\" for a in sorted_hill]\naxes[0, 0].barh(sorted_hill.index, sorted_hill.values, color=colors_hill, edgecolor=\"black\", linewidth=0.3)\naxes[0, 0].axvline(2, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"alpha=2\")\naxes[0, 0].axvline(4, color=\"green\", linestyle=\"--\", alpha=0.7, label=\"alpha=4\")\naxes[0, 0].set_xlabel(\"Hill alpha\")\naxes[0, 0].set_title(\"Hill Tail Index (sorted)\")\naxes[0, 0].legend(fontsize=8)\naxes[0, 0].tick_params(axis=\"y\", labelsize=8)\n\n# Panel 2: Hurst H sorted bar chart\nsorted_hurst = fred_all[\"Hurst H\"].sort_values()\ncolors_hurst = [\"#2196F3\" if h > 0.5 else \"#9E9E9E\" for h in sorted_hurst]\naxes[0, 1].barh(sorted_hurst.index, sorted_hurst.values, color=colors_hurst, edgecolor=\"black\", linewidth=0.3)\naxes[0, 1].axvline(0.5, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"H=0.5\")\naxes[0, 1].set_xlabel(\"Hurst H\")\naxes[0, 1].set_title(\"Hurst Exponent (sorted)\")\naxes[0, 1].legend(fontsize=8)\naxes[0, 1].tick_params(axis=\"y\", labelsize=8)\n\n# Panel 3: VaR 99% and ES 99% bar chart\nvar_data = fred_all[[\"VaR 99%\", \"ES 99%\"]].dropna().sort_values(\"ES 99%\", ascending=False).head(15)\nx = np.arange(len(var_data))\nwidth = 0.4\naxes[1, 0].barh(var_data.index, var_data[\"VaR 99%\"], height=width, label=\"VaR 99%\", color=\"#FF5722\", alpha=0.8)\naxes[1, 0].barh([i + width for i in range(len(var_data))], var_data[\"ES 99%\"], height=width, label=\"ES 99%\", color=\"#B71C1C\", alpha=0.8)\naxes[1, 0].set_xlabel(\"Loss (log returns)\")\naxes[1, 0].set_title(\"GPD Tail Risk: Top 15 by ES\")\naxes[1, 0].legend(fontsize=8)\naxes[1, 0].tick_params(axis=\"y\", labelsize=8)\n\n# Panel 4: DEH gamma sorted bar chart\nsorted_deh = fred_all[\"DEH gamma\"].sort_values(ascending=False)\ncolors_deh = [\"#E53935\" if g > 0.2 else \"#FF9800\" if g > 0 else \"#43A047\" for g in sorted_deh]\naxes[1, 1].barh(sorted_deh.index, sorted_deh.values, color=colors_deh, edgecolor=\"black\", linewidth=0.3)\naxes[1, 1].axvline(0, color=\"gray\", linestyle=\"-\", alpha=0.5)\naxes[1, 1].set_xlabel(\"DEH gamma\")\naxes[1, 1].set_title(\"DEH Extreme Value Index (sorted)\")\naxes[1, 1].tick_params(axis=\"y\", labelsize=8)\n\nplt.suptitle(\"FRED Forex: 23 Currency Pairs — Key Metrics\", fontsize=14, y=1.01)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1rjbu4l4tuh",
   "source": "# Summary statistics across all FRED pairs\nfred_summary = fred_all.drop(columns=[\"N\"]).agg([\"mean\", \"median\", \"std\", \"min\", \"max\"])\n\nprint(\"Summary Statistics: All FRED Forex Pairs\")\nprint(\"=\" * 100)\ndisplay(fred_summary.style.format(\"{:.4f}\"))\n\n# Comparison: FRED forex averages vs the 4 sample assets\nprint(\"\\n\\nComparison: Sample Assets vs FRED Forex Mean\")\nprint(\"-\" * 60)\n\nsample_metrics = {}\nfor name in assets:\n    r = returns[name]\n    l = negative_returns(r)\n    sample_metrics[name] = {\n        \"Hill alpha\": hill_estimator(l),\n        \"DEH gamma\": deh_estimator(l),\n        \"Hurst H\": hurst_exponent(r),\n    }\n\nsample_metrics[\"FRED Mean\"] = {\n    \"Hill alpha\": fred_all[\"Hill alpha\"].mean(),\n    \"DEH gamma\": fred_all[\"DEH gamma\"].mean(),\n    \"Hurst H\": fred_all[\"Hurst H\"].mean(),\n}\n\ncomp_df = pd.DataFrame(sample_metrics).T\nprint(comp_df.to_string(float_format=lambda x: f\"{x:.3f}\"))\n\nprint(\"\\n\\nKey finding: FRED forex pairs have tail indices comparable to\")\nprint(\"traditional assets (SPY, Gold) — confirming that fat tails are\")\nprint(\"universal across financial markets, not just a crypto phenomenon.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "| Method | Measures | Interpretation | Best for |\n",
    "|--------|----------|----------------|----------|\n",
    "| **Hill estimator** | Tail index alpha | Lower alpha = fatter tails; alpha<2 infinite variance | Classic tail index estimation |\n",
    "| **DEH estimator** | Tail index alpha | Same as Hill but more robust to second-order bias | When Hill is unreliable |\n",
    "| **QQ estimator** | Tail index alpha | Log-log QQ regression-based alpha | Visual diagnostic + estimation |\n",
    "| **Pickands estimator** | Tail index gamma | gamma>0 indicates fat tails; distribution-free | Non-parametric tail detection |\n",
    "| **Max-stability Kappa** | Departure from Gaussian max-stability | Kappa below benchmark = fat tails | Quick Gaussian vs non-Gaussian test |\n",
    "| **Taleb Kappa** | Same concept, reversed convention | Kappa above benchmark = fat tails | Taleb's formulation for practitioners |\n",
    "| **Max-to-Sum Ratio** | Concentration of extremes | Higher ratio = single events dominate | Detecting infinite-variance regimes |\n",
    "| **GPD fit** | Exceedance distribution (sigma, xi) | xi>0 = power-law tail | Threshold-based tail modelling |\n",
    "| **GPD VaR/ES** | Value-at-Risk, Expected Shortfall | Tail risk quantification | Risk management |\n",
    "| **GEV fit** | Block maxima distribution (mu, sigma, xi) | xi>0 = Frechet (fat tail) | Characterizing worst-case blocks |\n",
    "| **Hurst exponent** | Long-range dependence H | H>0.5 persistent, H<0.5 mean-reverting | Detecting trending vs mean-reverting |\n",
    "| **DFA exponent** | Detrended fluctuation scaling | Similar to Hurst, robust to trends | Persistence in non-stationary data |\n",
    "| **Spectral exponent** | Power spectrum scaling beta | Higher = more persistence | Frequency-domain persistence analysis |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mhbpgb05hgg",
   "source": "## 12. Crash Detection Accuracy: Precision, Recall & F1\n\nThe methods above measure **static properties** of returns (tail thickness, persistence).\nBut the real question is: **can they predict crashes?**\n\nTo answer this we run each method on **crash windows** (the 120 days before a known\ndrawdown peak) and **non-crash windows** (random 120-day stretches far from any crash).\nThis gives us a binary classification problem, and we measure it with three standard metrics.\n\n### The confusion matrix\n\nEvery prediction falls into one of four boxes:\n\n```\n                        Actual crash?\n                      YES           NO\n                 ┌───────────┬───────────┐\n  Method    YES  │    TP     │    FP     │\n  fired?         │ (caught!) │ (false    │\n                 │           │  alarm)   │\n                 ├───────────┼───────────┤\n            NO   │    FN     │    TN     │\n                 │ (missed!) │ (correct  │\n                 │           │  silence) │\n                 └───────────┴───────────┘\n```\n\n- **TP** (True Positive): crash was coming, method fired — good\n- **FP** (False Positive): no crash, method fired anyway — false alarm\n- **FN** (False Negative): crash was coming, method stayed silent — missed it\n- **TN** (True Negative): no crash, method stayed silent — good\n\n### The three metrics\n\n| Metric | Formula | Question it answers |\n|--------|---------|---------------------|\n| **Precision** | TP / (TP + FP) | \"When the alarm goes off, how often is it real?\" |\n| **Recall** | TP / (TP + FN) | \"Of all actual crashes, how many did we catch?\" |\n| **F1** | 2 * P * R / (P + R) | \"Single score balancing both concerns\" |\n\n**The tradeoff**: you can trivially get 100% recall by always screaming \"CRASH!\" (but\nprecision goes to zero). You can get 100% precision by never firing (but recall goes to\nzero). F1 is the harmonic mean — it punishes methods that are good at one but terrible\nat the other.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pb0klkert3l",
   "source": "from analysis.accuracy_report import (\n    find_drawdowns, sample_non_crash_windows,\n    test_method_on_drawdown, test_method_on_non_crash,\n    compute_metrics,\n)\n\n# ── Run the same evaluation as accuracy_report.py ──\ntp_records = []\nfp_records = []\n\nall_crash_events = {}\nfor asset_name, threshold in [(\"btc\", 0.15), (\"spy\", 0.08), (\"gold\", 0.08)]:\n    df = assets[{\"btc\": \"BTC\", \"spy\": \"SPY\", \"gold\": \"Gold\"}[asset_name]]\n    events = find_drawdowns(df, min_dd=threshold, min_apart=90)\n    all_crash_events[asset_name] = events\n\n    for ev in events:\n        res, _ = test_method_on_drawdown(df, ev[\"peak_idx\"])\n        if res is None:\n            continue\n        for method, detected in res.items():\n            if detected is not None:\n                tp_records.append((method, detected))\n\nfor asset_name in [\"btc\", \"spy\", \"gold\"]:\n    df = assets[{\"btc\": \"BTC\", \"spy\": \"SPY\", \"gold\": \"Gold\"}[asset_name]]\n    events = all_crash_events[asset_name]\n    for nc in sample_non_crash_windows(df, events, n_samples=50, seed=42):\n        res, _ = test_method_on_non_crash(df, nc[\"center_idx\"])\n        if res is None:\n            continue\n        for method, fired in res.items():\n            if fired is not None:\n                fp_records.append((method, fired))\n\nclassical_methods = [\n    \"lppls\", \"lppls_confidence\", \"gsadf\", \"hurst\", \"dfa\",\n    \"kappa\", \"taleb_kappa\", \"pickands\", \"deh\", \"qq\",\n    \"gpd_var\", \"maxsum\", \"spectral\", \"hill\",\n]\nmetrics = compute_metrics(tp_records, fp_records, classical_methods)\n\nn_crashes = sum(1 for m, d in tp_records if m == \"lppls\" and d is not None)\nn_noncrash = sum(1 for m, d in fp_records if m == \"lppls\" and d is not None)\nprint(f\"Evaluated on {n_crashes} crash windows and {n_noncrash} non-crash windows (BTC, SPY, Gold).\\n\")\nprint(\"All numbers are in-sample on historical data. This is not financial advice.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pru5k4tq5c",
   "source": "# ── Precision / Recall / F1 table ──\nsorted_methods = sorted(classical_methods, key=lambda m: metrics.get(m, {}).get(\"f1\", 0), reverse=True)\n\nrows = []\nfor method in sorted_methods:\n    m = metrics[method]\n    if (m[\"tp\"] + m[\"fn\"]) == 0:\n        continue\n    rows.append({\n        \"Method\": method,\n        \"TP\": m[\"tp\"], \"FP\": m[\"fp\"], \"FN\": m[\"fn\"], \"TN\": m[\"tn\"],\n        \"Precision\": m[\"precision\"], \"Recall\": m[\"recall\"], \"F1\": m[\"f1\"],\n    })\n\nmetrics_df = pd.DataFrame(rows)\n\n# Format percentages for display\ndisplay_df = metrics_df.copy()\nfor col in [\"Precision\", \"Recall\", \"F1\"]:\n    display_df[col] = display_df[col].map(lambda x: f\"{x:.0%}\")\nprint(display_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gy0gwr9np7f",
   "source": "# ── Visual: Precision vs Recall scatter ──\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left: Precision vs Recall scatter\nax = axes[0]\nfor _, row in metrics_df.iterrows():\n    ax.scatter(row[\"Recall\"], row[\"Precision\"], s=100, zorder=3)\n    ax.annotate(row[\"Method\"], (row[\"Recall\"], row[\"Precision\"]),\n                textcoords=\"offset points\", xytext=(6, 4), fontsize=8)\n\n# F1 iso-lines\nfor f1_val in [0.2, 0.4, 0.6, 0.8]:\n    r_range = np.linspace(0.01, 1, 200)\n    p_range = f1_val * r_range / (2 * r_range - f1_val)\n    mask = (p_range > 0) & (p_range <= 1)\n    ax.plot(r_range[mask], p_range[mask], '--', color='grey', alpha=0.4, linewidth=0.8)\n    # Label the iso-line\n    idx = np.argmin(np.abs(r_range - 0.95))\n    if mask[idx]:\n        ax.text(0.96, p_range[idx], f\"F1={f1_val:.1f}\", fontsize=7, color='grey', va='center')\n\nax.set_xlabel(\"Recall (fraction of crashes caught)\")\nax.set_ylabel(\"Precision (fraction of alerts that are correct)\")\nax.set_title(\"Precision vs Recall (dashed lines = F1 iso-curves)\")\nax.set_xlim(0, 1.05)\nax.set_ylim(0, 0.7)\n\n# Right: F1 bar chart\nax2 = axes[1]\ncolors = ['#2196F3' if f1 > 0.4 else '#FF9800' if f1 > 0.25 else '#9E9E9E'\n          for f1 in metrics_df[\"F1\"]]\nax2.barh(metrics_df[\"Method\"], metrics_df[\"F1\"], color=colors)\nax2.set_xlabel(\"F1 Score\")\nax2.set_title(\"F1 Score by Method (sorted best to worst)\")\nax2.invert_yaxis()\nfor i, (f1, prec, rec) in enumerate(zip(metrics_df[\"F1\"], metrics_df[\"Precision\"], metrics_df[\"Recall\"])):\n    ax2.text(f1 + 0.01, i, f\"P={prec:.0%} R={rec:.0%}\", va='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ndtxz6sxwya",
   "source": "# ── Confusion matrix heatmap for LPPLS (the best method) ──\nlppls_m = metrics[\"lppls\"]\n\ncm = np.array([\n    [lppls_m[\"tp\"], lppls_m[\"fp\"]],\n    [lppls_m[\"fn\"], lppls_m[\"tn\"]],\n])\n\nfig, ax = plt.subplots(figsize=(6, 5))\nim = ax.imshow(cm, cmap=\"YlOrRd\", aspect=\"auto\")\n\nlabels = [\n    [f\"TP\\n{cm[0,0]}\\n(caught!)\", f\"FP\\n{cm[0,1]}\\n(false alarm)\"],\n    [f\"FN\\n{cm[1,0]}\\n(missed!)\", f\"TN\\n{cm[1,1]}\\n(correct silence)\"],\n]\n\nfor i in range(2):\n    for j in range(2):\n        color = \"white\" if cm[i, j] > cm.max() * 0.6 else \"black\"\n        ax.text(j, i, labels[i][j], ha=\"center\", va=\"center\",\n                fontsize=12, fontweight=\"bold\", color=color)\n\nax.set_xticks([0, 1])\nax.set_xticklabels([\"Actual: CRASH\", \"Actual: NO CRASH\"])\nax.set_yticks([0, 1])\nax.set_yticklabels([\"Predicted:\\nCRASH\", \"Predicted:\\nNO CRASH\"])\nax.set_title(f\"LPPLS Confusion Matrix\\nP={lppls_m['precision']:.0%}  R={lppls_m['recall']:.0%}  F1={lppls_m['f1']:.0%}\")\nplt.tight_layout()\nplt.show()\n\nprint(\"Reading the confusion matrix:\")\nprint(f\"  - LPPLS correctly catches {lppls_m['tp']} of {lppls_m['tp']+lppls_m['fn']} crashes (recall = {lppls_m['recall']:.0%})\")\nprint(f\"  - But it also fires {lppls_m['fp']} times when there's no crash (precision = {lppls_m['precision']:.0%})\")\nprint(f\"  - For crash detection, high recall is arguably more important than high precision:\")\nprint(f\"    missing a crash (FN) is worse than a false alarm (FP).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vlqgjmmipfb",
   "source": "### Why LPPLS dominates\n\nLPPLS (Log-Periodic Power Law Singularity) is the only method here that models the\n**mechanism** of a crash: super-exponential price growth with accelerating oscillations\nconverging to a critical time tc. The other methods detect **symptoms** (fat tails,\npersistence, regime shifts) that are necessary but not sufficient for an imminent crash.\n\nThis is why LPPLS achieves 90% recall with the best F1 — it's the only method that\ndirectly models the bubble-to-crash transition, rather than just measuring statistical\nproperties of returns.\n\n### Why no method has high precision\n\nAll methods have precision below 55%. This is fundamental, not a bug:\n- Markets can exhibit bubble-like patterns (super-exponential growth, fat tails,\n  persistence) without crashing — the bubble can deflate slowly instead\n- The same statistical signatures appear in bull markets that continue for years\n- Crash timing is inherently uncertain — a method that fires \"too early\" registers\n  as a false positive even though the underlying signal was correct\n\nThis is why the aggregator approach matters: combining independent signal categories\n(bubble dynamics, tail risk, persistence, structural) reduces false positives when\nmultiple categories agree.\n\n*All numbers are in-sample on historical data. This is not financial advice.*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ssfyrwnicon",
   "source": "## 13. Beyond Valuations: Applicability to Revenue & Profit\n\nAll the methods above were tested on **market prices** (BTC, SPY, Gold, FX). But do they\nwork on **fundamental data** like revenue or profit growth?\n\nThe short answer: **some do, some don't.** It depends on whether the method measures a\ngeneral statistical property of time series or a market-specific mechanism.\n\n### Which methods transfer\n\n| Method | Transfers? | Why |\n|--------|-----------|-----|\n| **Hill, DEH, QQ, Pickands** (tail index) | Yes | Tail thickness is a property of any distribution. Revenue growth rates have fat tails too — Gabaix (2011) showed that idiosyncratic firm-level shocks (granular origins) drive aggregate fluctuations precisely because firm-size distributions are fat-tailed. |\n| **Kappa, Taleb kappa** | Yes | Measures departure from Gaussian max-stability. Works on any data where you suspect non-Gaussian extremes. |\n| **Max-to-Sum ratio** | Yes | Tests whether single observations dominate the total. A single quarter where revenue drops 50% dominating the sum = same math as a single market crash day. |\n| **GPD / GEV** | Yes | EVT is distribution-agnostic. Fitting GPD to the worst quarterly revenue declines gives valid VaR/ES estimates for fundamental risk. |\n| **Hurst, DFA, Spectral** (persistence) | Yes | Revenue series are often *strongly* persistent (H > 0.5) due to contracts, customer stickiness, and operating leverage. A shift from persistent to anti-persistent (H dropping below 0.5) could signal fundamental deterioration. |\n| **GSADF** (explosive roots) | Partially | Detects unsustainable exponential growth. Applied to quarterly revenue, it could flag \"revenue bubbles\" in hypergrowth companies — growth rates that imply the company would need to capture 100% of its TAM. |\n| **LPPLS** | No | Models speculative bubble dynamics (herding, positive feedback, log-periodic oscillations). Revenue doesn't exhibit these patterns — it's driven by real economic activity, not reflexive speculation. |\n| **LPPLS confidence** | No | Same limitation as LPPLS. |\n\n### The key distinction\n\n**Market prices** reflect collective speculative behavior — they have reflexivity\n(Soros), herding, and positive feedback loops. This is what LPPLS models.\n\n**Revenue/profit** reflects real economic activity — customer demand, operational\nexecution, competitive dynamics. The feedback loops are different: a company's revenue\ndoesn't go up because investors *believe* it will go up (unlike stock prices).\n\nThis means:\n- **Tail estimators** work on both because fat tails are universal (Mandelbrot, Taleb)\n- **Persistence methods** work on both because autocorrelation is a general property\n- **Bubble detectors** (LPPLS, GSADF) are price-specific — they need speculative dynamics\n\n### What you'd measure in practice\n\nFor a company's **quarterly revenue** time series:\n1. **Log-changes**: `r_t = log(revenue_t / revenue_{t-1})` — the growth rate per quarter\n2. **Tail index**: Hill/DEH alpha on the growth rates — are revenue shocks fat-tailed?\n3. **Persistence**: Hurst/DFA on growth rates — is growth momentum persistent or mean-reverting?\n4. **EVT**: GPD on the worst declines — what's the 95th percentile revenue drop?\n5. **GSADF**: on the revenue level — is growth explosive/unsustainable?\n\nThe challenge: quarterly data gives you ~80 observations over 20 years (vs ~5,000 daily\nprices). Tail estimators need at least ~100 data points to be reliable, so you may need\nmonthly revenue or longer history.\n\n*This is not financial advice. For research and educational purposes only.*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}