{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 07 - Neural Network LPPLS Methods\n\nThree neural network approaches to LPPLS bubble detection from recent papers:\n\n1. **M-LNN** (Mono-LPPLS NN) — one network trained per series, minimizes reconstruction MSE\n2. **P-LNN** (Poly-LPPLS NN) — pre-trained on 100K synthetic series, ~700x faster than CMA-ES\n3. **HLPPL** (Hyped LPPL) — dual-stream transformer fusing price + sentiment features\n\nAll three share a physics-informed design: the network predicts nonlinear LPPLS\nparameters (tc, m, omega), then linear parameters (A, B, C1, C2) are solved\nanalytically via OLS. This makes training stable and predictions interpretable.\n\n**References:**\n- Nielsen, Sornette & Raissi (2024), [arXiv:2405.12803](https://arxiv.org/abs/2405.12803)\n- Cao, Shao, Yan & Geman (2025), [arXiv:2510.10878](https://arxiv.org/abs/2510.10878)\n\n**Requirements:** `pip install fatcrash[deep]` (adds PyTorch)\n\n> **DISCLAIMER:** This software is for academic research and educational purposes only.\n> It does not constitute financial advice."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom fatcrash.data.ingest import from_sample\nfrom fatcrash.data.transforms import log_prices, time_index\nfrom fatcrash.indicators.lppls_indicator import fit_lppls\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (14, 6)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load BTC data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = from_sample(\"btc\")\ndf = time_index(df)\ndf[\"log_price\"] = log_prices(df[\"close\"].values)\n\n# Focus on the 2017 bubble for demonstrations\nbubble_mask = (df.index >= \"2017-01-01\") & (df.index <= \"2018-02-06\")\nbubble_df = df[bubble_mask].copy()\n\ntimes = np.arange(len(bubble_df), dtype=np.float64)\nlp = bubble_df[\"log_price\"].values\n\nprint(f\"Bubble window: {len(bubble_df)} days ({bubble_df.index[0].date()} to {bubble_df.index[-1].date()})\")\n\nfig, ax = plt.subplots(figsize=(14, 5))\nax.plot(bubble_df.index, bubble_df[\"close\"], color=\"steelblue\")\nax.set_title(\"BTC/USD — 2017 Bubble\")\nax.set_ylabel(\"Price (USD)\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. M-LNN — Mono-LPPLS Neural Network\n\nOne small network (2 hidden layers, 64 units) trained **per time series**.\nThe network predicts (tc, m, omega) and the loss is the MSE between the LPPLS\nreconstruction and observed prices. No pre-training needed.\n\nArchitecture: `Linear(N, 64) → ReLU → Linear(64, 64) → ReLU → Linear(64, 3)`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fatcrash.nn.mlnn import fit_mlnn\n\nmlnn_result = fit_mlnn(times, lp, epochs=200, lr=1e-2, seed=42)\n\nprint(\"=== M-LNN Result ===\")\nprint(f\"  tc       = {mlnn_result.tc:.1f} (day index)\")\nprint(f\"  m        = {mlnn_result.m:.4f}\")\nprint(f\"  omega    = {mlnn_result.omega:.4f}\")\nprint(f\"  B        = {mlnn_result.b:.6f}\")\nprint(f\"  RSS      = {mlnn_result.rss:.6f}\")\nprint(f\"  is_bubble = {mlnn_result.is_bubble}\")\nprint(f\"  confidence = {mlnn_result.confidence:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Classical LPPLS for comparison\n\nCMA-ES optimizer in Rust — the traditional approach."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "classical = fit_lppls(times, lp)\n\nprint(\"=== Classical LPPLS (CMA-ES) ===\")\nprint(f\"  tc       = {classical.tc:.1f}\")\nprint(f\"  m        = {classical.m:.4f}\")\nprint(f\"  omega    = {classical.omega:.4f}\")\nprint(f\"  B        = {classical.b:.6f}\")\nprint(f\"  RSS      = {classical.rss:.6f}\")\nprint(f\"  is_bubble = {classical.is_bubble}\")\n\nprint(\"\\n=== Comparison ===\")\nprint(f\"  {'Param':<10s} {'Classical':>12s} {'M-LNN':>12s}\")\nprint(f\"  {'-'*34}\")\nprint(f\"  {'tc':<10s} {classical.tc:>12.1f} {mlnn_result.tc:>12.1f}\")\nprint(f\"  {'m':<10s} {classical.m:>12.4f} {mlnn_result.m:>12.4f}\")\nprint(f\"  {'omega':<10s} {classical.omega:>12.4f} {mlnn_result.omega:>12.4f}\")\nprint(f\"  {'RSS':<10s} {classical.rss:>12.6f} {mlnn_result.rss:>12.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. P-LNN — Poly-LPPLS Neural Network\n\nPre-trained on synthetic LPPLS data, ~700x faster than CMA-ES at inference.\nA deeper network (4 hidden layers: 512→256→128→64) maps min-max normalized\nprices to (tc, m, omega) in a single forward pass.\n\nWe train on a small dataset here for demonstration. For production use,\ntrain on 100K+ samples."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fatcrash.nn.plnn import train_plnn, predict_plnn\nfrom fatcrash.nn.synthetic import generate_dataset\nimport time\n\n# Train P-LNN on a small synthetic dataset (1000 samples for demo speed)\nprint(\"Training P-LNN on 1000 synthetic LPPLS series...\")\nt0 = time.time()\nplnn_model = train_plnn(\n    variant=\"P-LNN-demo\",\n    n_samples=1000,\n    n_obs=252,\n    batch_size=8,\n    epochs=5,\n    lr=1e-4,\n    seed=42,\n)\ntrain_time = time.time() - t0\nprint(f\"Training took {train_time:.1f}s\")\n\n# Inference on real data\nt0 = time.time()\nplnn_result = predict_plnn(plnn_model, times, lp, window_size=252)\ninfer_time = time.time() - t0\n\nprint(f\"\\n=== P-LNN Result (inference: {infer_time*1000:.1f}ms) ===\")\nprint(f\"  tc       = {plnn_result.tc:.1f}\")\nprint(f\"  m        = {plnn_result.m:.4f}\")\nprint(f\"  omega    = {plnn_result.omega:.4f}\")\nprint(f\"  B        = {plnn_result.b:.6f}\")\nprint(f\"  RSS      = {plnn_result.rss:.6f}\")\nprint(f\"  is_bubble = {plnn_result.is_bubble}\")\nprint(f\"  confidence = {plnn_result.confidence:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Synthetic data visualization\n\nThe P-LNN is trained on synthetic LPPLS series with controlled noise.\nLet's visualize what the training data looks like."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fatcrash.nn.synthetic import generate_lppls_series, add_white_noise, add_ar1_noise\n\nrng = np.random.default_rng(42)\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\n\nfor i in range(3):\n    clean, params = generate_lppls_series(n_obs=252, rng=rng)\n    noisy_white = add_white_noise(clean, alpha=0.05, rng=rng)\n    noisy_ar1 = add_ar1_noise(clean, phi=0.9, amplitude=0.03, rng=rng)\n\n    axes[0, i].plot(clean, \"k-\", linewidth=0.8, label=\"Clean\")\n    axes[0, i].plot(noisy_white, \"b-\", alpha=0.5, linewidth=0.5, label=\"+ White noise\")\n    axes[0, i].set_title(f\"tc={params['tc']:.0f}, m={params['m']:.2f}, ω={params['omega']:.1f}\")\n    if i == 0:\n        axes[0, i].legend(fontsize=8)\n        axes[0, i].set_ylabel(\"White noise\")\n\n    axes[1, i].plot(clean, \"k-\", linewidth=0.8, label=\"Clean\")\n    axes[1, i].plot(noisy_ar1, \"r-\", alpha=0.5, linewidth=0.5, label=\"+ AR(1) noise\")\n    if i == 0:\n        axes[1, i].legend(fontsize=8)\n        axes[1, i].set_ylabel(\"AR(1) noise\")\n\nfig.suptitle(\"Synthetic LPPLS Training Data\", fontsize=13)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. HLPPL — Dual-stream transformer with sentiment\n\nHLPPL (Cao et al. 2025) fuses price dynamics with market hype signals:\n\n- **Stream 1 (Temporal):** log-price, returns, volatility, volume change, hype index → 2-layer TransformerEncoder\n- **Stream 2 (Sentiment):** volume z-score, momentum, absolute return z-score, hype index → 1-layer TransformerEncoder\n\nThe two streams are concatenated and projected to a crash probability [0, 1].\n\nWe train on synthetic labeled data for demonstration."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from fatcrash.nn.sentiment import compute_sentiment_proxy\n\n# Compute sentiment proxy from BTC OHLCV data\nsent = compute_sentiment_proxy(df, volume_col=\"volume\", close_col=\"close\")\n\nfig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n\naxes[0].plot(df.index, df[\"close\"], color=\"steelblue\", linewidth=0.5)\naxes[0].set_ylabel(\"Price\")\naxes[0].set_title(\"BTC/USD Price & Volume-Based Sentiment Proxy\")\n\naxes[1].plot(df.index, sent.volume_z, color=\"purple\", linewidth=0.3, alpha=0.7)\naxes[1].set_ylabel(\"Volume Z\")\naxes[1].axhline(0, color=\"gray\", linewidth=0.5)\n\naxes[2].plot(df.index, sent.abs_return_z, color=\"darkred\", linewidth=0.3, alpha=0.7)\naxes[2].set_ylabel(\"|Return| Z\")\naxes[2].axhline(0, color=\"gray\", linewidth=0.5)\n\naxes[3].plot(df.index, sent.hype_index, color=\"orange\", linewidth=0.5)\naxes[3].set_ylabel(\"Hype Index\")\naxes[3].set_ylim(0, 1)\naxes[3].axhline(0.5, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fatcrash.nn.hlppl import train_hlppl, predict_hlppl\n\n# Create labeled training samples from BTC data\n# Label: 1 if >15% drawdown within 30 days, 0 otherwise\nwindow = 60\ntrain_dfs = []\nlabels = []\n\nclose = df[\"close\"].values\nfor i in range(window, len(close) - 30, window):\n    chunk = df.iloc[i - window:i].copy()\n    # Check for drawdown in next 30 days\n    future = close[i:i + 30]\n    max_drawdown = (future.min() - close[i]) / close[i] if len(future) > 0 else 0\n    is_crash = 1 if max_drawdown < -0.15 else 0\n    train_dfs.append(chunk)\n    labels.append(is_crash)\n\nprint(f\"Training samples: {len(train_dfs)} ({sum(labels)} crash, {len(labels) - sum(labels)} normal)\")\n\n# Train HLPPL (small epochs for demo)\nhlppl_model = train_hlppl(\n    train_dfs, labels,\n    window=window, epochs=10, lr=1e-3, seed=42,\n)\nprint(\"HLPPL training complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run HLPPL on the 2017 bubble window\nhlppl_result = predict_hlppl(hlppl_model, bubble_df, window=window)\n\nprint(f\"=== HLPPL Result ===\")\nprint(f\"  bubble_score = {hlppl_result.bubble_score:.4f}\")\nprint(f\"  temporal features shape  = {hlppl_result.temporal_features.shape}\")\nprint(f\"  sentiment features shape = {hlppl_result.sentiment_features.shape}\")\n\n# Run HLPPL on rolling windows across full dataset\nscores = []\nscore_dates = []\nfor i in range(window, len(df), 5):\n    chunk = df.iloc[i - window:i]\n    r = predict_hlppl(hlppl_model, chunk, window=window)\n    scores.append(r.bubble_score)\n    score_dates.append(df.index[i - 1])\n\nscores = np.array(scores)\nscore_dates = pd.DatetimeIndex(score_dates)\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\naxes[0].plot(df.index, df[\"close\"], color=\"steelblue\", linewidth=0.5)\naxes[0].set_ylabel(\"Price (USD)\")\naxes[0].set_title(\"BTC/USD — HLPPL Rolling Bubble Score\")\n\naxes[1].plot(score_dates, scores, color=\"crimson\", linewidth=0.5)\naxes[1].axhline(0.5, color=\"gray\", linewidth=0.5, linestyle=\"--\")\naxes[1].set_ylabel(\"Bubble Score\")\naxes[1].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Signal integration\n\nAll three NN methods produce signals that feed into the aggregator alongside\nthe 13 classical methods."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fatcrash.aggregator.signals import (\n    mlnn_signal, plnn_signal, hlppl_signal, aggregate_signals,\n)\n\n# Convert NN results to [0, 1] signals\ns_mlnn = mlnn_signal(mlnn_result.confidence, mlnn_result.is_bubble)\ns_plnn = plnn_signal(plnn_result.confidence, plnn_result.is_bubble)\ns_hlppl = hlppl_signal(hlppl_result.bubble_score)\n\nprint(\"=== NN Signal Values ===\")\nprint(f\"  M-LNN signal  = {s_mlnn:.4f} (is_bubble={mlnn_result.is_bubble})\")\nprint(f\"  P-LNN signal  = {s_plnn:.4f} (is_bubble={plnn_result.is_bubble})\")\nprint(f\"  HLPPL signal  = {s_hlppl:.4f}\")\n\n# Example aggregation with just the NN signals\ncomponents = {\n    \"mlnn_signal\": s_mlnn,\n    \"plnn_signal\": s_plnn,\n    \"hlppl_signal\": s_hlppl,\n}\ncrash = aggregate_signals(components)\nprint(f\"\\n=== Aggregated (NN only) ===\")\nprint(f\"  probability = {crash.probability:.4f}\")\nprint(f\"  level       = {crash.level}\")\nprint(f\"  n_agreeing  = {crash.n_agreeing} categories\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Save and load models\n\nBoth P-LNN and HLPPL support saving/loading trained weights for production use."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import tempfile, os\nfrom fatcrash.nn.plnn import load_plnn\nfrom fatcrash.nn.hlppl import save_hlppl, load_hlppl\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    # P-LNN save/load\n    plnn_path = os.path.join(tmpdir, \"plnn_demo.pt\")\n    plnn_model.save(plnn_path)\n    plnn_loaded = load_plnn(plnn_path)\n    r = predict_plnn(plnn_loaded, times, lp, window_size=252)\n    print(f\"P-LNN save/load roundtrip OK — tc={r.tc:.1f}, m={r.m:.4f}\")\n\n    # HLPPL save/load\n    hlppl_path = os.path.join(tmpdir, \"hlppl_demo.pt\")\n    save_hlppl(hlppl_model, hlppl_path)\n    hlppl_loaded = load_hlppl(hlppl_path)\n    r2 = predict_hlppl(hlppl_loaded, bubble_df, window=window)\n    print(f\"HLPPL save/load roundtrip OK — bubble_score={r2.bubble_score:.4f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}