{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08 - Historical Backtest\n",
    "\n",
    "The ultimate test of a crash detection system is whether it would have fired\n",
    "meaningful warnings before **known crashes**. This notebook runs the full\n",
    "fatcrash pipeline against historical episodes using **bundled sample data**\n",
    "(no internet required).\n",
    "\n",
    "| Episode            | Asset  | Peak Date    | Drawdown  |\n",
    "|--------------------|--------|--------------|-----------|\n",
    "| 2017 BTC bubble    | BTC    | 2017-12-17   | ~-84%     |\n",
    "| 2021 BTC bubble    | BTC    | 2021-11-10   | ~-77%     |\n",
    "| 2008 Financial Crisis | SPY | 2007-10-09   | ~-57%     |\n",
    "| 2020 COVID crash   | SPY    | 2020-02-19   | ~-34%     |\n",
    "| 2011 Gold peak     | Gold   | 2011-09-06   | ~-45%     |\n",
    "\n",
    "For each episode we:\n",
    "1. Compute all indicators using only data available **before** the crash\n",
    "2. Build the composite crash signal at each timestep\n",
    "3. Evaluate whether the signal was elevated in the 30-60 days before the peak\n",
    "\n",
    "> **DISCLAIMER:** This software is for academic research and educational purposes only.\n",
    "> It does not constitute financial advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from fatcrash.data.ingest import from_sample\n",
    "from fatcrash.data.transforms import log_returns, log_prices, time_index\n",
    "from fatcrash.indicators.lppls_indicator import compute_confidence\n",
    "from fatcrash.indicators.tail_indicator import rolling_tail_index, rolling_kappa\n",
    "from fatcrash.indicators.evt_indicator import rolling_var_es\n",
    "from fatcrash.aggregator.signals import (\n",
    "    aggregate_signals,\n",
    "    lppls_confidence_signal,\n",
    "    tc_proximity_signal,\n",
    "    var_exceedance_signal,\n",
    "    kappa_regime_signal,\n",
    "    hill_thinning_signal,\n",
    ")\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load sample data and define crash episodes\n",
    "\n",
    "We use the bundled sample datasets (BTC, SPY, Gold) so the notebook runs\n",
    "offline. Each episode slices the relevant date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sample datasets\n",
    "data_btc = from_sample(\"btc\")\n",
    "data_spy = from_sample(\"spy\")\n",
    "data_gold = from_sample(\"gold\")\n",
    "\n",
    "# Load known crash dates for reference\n",
    "with open(Path(\"../data/sample/known_crashes.json\")) as f:\n",
    "    known = json.load(f)\n",
    "\n",
    "print(f\"BTC:  {len(data_btc)} days, {data_btc.index[0].date()} to {data_btc.index[-1].date()}\")\n",
    "print(f\"SPY:  {len(data_spy)} days, {data_spy.index[0].date()} to {data_spy.index[-1].date()}\")\n",
    "print(f\"Gold: {len(data_gold)} days, {data_gold.index[0].date()} to {data_gold.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_episodes = [\n",
    "    {\n",
    "        \"name\": \"2017 BTC Bubble\",\n",
    "        \"data\": data_btc,\n",
    "        \"data_start\": \"2015-01-01\",\n",
    "        \"peak_date\": \"2017-12-17\",\n",
    "        \"plot_end\": \"2018-06-01\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"2021 BTC Bubble\",\n",
    "        \"data\": data_btc,\n",
    "        \"data_start\": \"2019-01-01\",\n",
    "        \"peak_date\": \"2021-11-10\",\n",
    "        \"plot_end\": \"2022-06-01\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"2008 Financial Crisis (SPY)\",\n",
    "        \"data\": data_spy,\n",
    "        \"data_start\": \"2003-01-01\",\n",
    "        \"peak_date\": \"2007-10-09\",\n",
    "        \"plot_end\": \"2009-06-01\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"2020 COVID Crash (SPY)\",\n",
    "        \"data\": data_spy,\n",
    "        \"data_start\": \"2017-01-01\",\n",
    "        \"peak_date\": \"2020-02-19\",\n",
    "        \"plot_end\": \"2020-09-01\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"2011 Gold Peak\",\n",
    "        \"data\": data_gold,\n",
    "        \"data_start\": \"2006-01-01\",\n",
    "        \"peak_date\": \"2011-09-06\",\n",
    "        \"plot_end\": \"2012-06-01\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for ep in crash_episodes:\n",
    "    df_slice = ep[\"data\"].loc[ep[\"data_start\"]:ep[\"plot_end\"]]\n",
    "    print(f\"{ep['name']:>35s}: {len(df_slice)} obs, peak {ep['peak_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Pipeline function\n",
    "\n",
    "Runs the fatcrash pipeline at each timestep: LPPLS confidence, EVT VaR,\n",
    "kappa regime, and Hill tail thinning. Converts each indicator to a [0,1]\n",
    "signal and aggregates into a composite crash probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(df, window=500):\n",
    "    \"\"\"\n",
    "    Run the full fatcrash pipeline on a DataFrame with 'close' column.\n",
    "    Returns a dict with dates, prices, per-timestep composite signal,\n",
    "    and individual component signal arrays.\n",
    "    \"\"\"\n",
    "    dates = df.index\n",
    "    prices = df[\"close\"].values\n",
    "    rets = log_returns(df)\n",
    "    lp = log_prices(df)\n",
    "    t = time_index(df)\n",
    "    n = len(dates)\n",
    "\n",
    "    # ---- Rolling indicators ----\n",
    "    # LPPLS confidence (returns conf, tc_mean, tc_std arrays)\n",
    "    conf_arr, tc_mean_arr, tc_std_arr = compute_confidence(\n",
    "        t, lp, n_windows=30, n_candidates=20,\n",
    "    )\n",
    "\n",
    "    # EVT: rolling VaR and ES (returns var_arr, es_arr)\n",
    "    var_arr, es_arr = rolling_var_es(rets, window=window)\n",
    "\n",
    "    # Kappa: rolling kappa (returns kappa_arr, benchmark)\n",
    "    kappa_arr, kappa_bench = rolling_kappa(rets, window=window)\n",
    "\n",
    "    # Hill: rolling tail index\n",
    "    hill_arr = rolling_tail_index(rets, window=window)\n",
    "\n",
    "    # ---- Convert to per-timestep signals ----\n",
    "    composite = np.full(n, np.nan)\n",
    "    comp_lppls = np.full(n, np.nan)\n",
    "    comp_tc = np.full(n, np.nan)\n",
    "    comp_var = np.full(n, np.nan)\n",
    "    comp_kappa = np.full(n, np.nan)\n",
    "    comp_hill = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(window + 1, n):\n",
    "        # LPPLS signals\n",
    "        c = conf_arr[i] if i < len(conf_arr) else np.nan\n",
    "        tc_m = tc_mean_arr[i] if i < len(tc_mean_arr) else np.nan\n",
    "        sig_lppls = lppls_confidence_signal(c) if not np.isnan(c) else 0.0\n",
    "\n",
    "        # tc proximity: days from current time to predicted tc\n",
    "        days_to_tc = tc_m - t[i] if not np.isnan(tc_m) else float(\"inf\")\n",
    "        sig_tc = tc_proximity_signal(days_to_tc)\n",
    "\n",
    "        # VaR exceedance (rets is 1 shorter than dates)\n",
    "        ret_i = rets[i - 1] if i - 1 < len(rets) else 0.0\n",
    "        v = var_arr[i - 1] if i - 1 < len(var_arr) else np.nan\n",
    "        sig_var = var_exceedance_signal(ret_i, v) if not np.isnan(v) else 0.0\n",
    "\n",
    "        # Kappa regime (kappa_arr aligned with rets)\n",
    "        k = kappa_arr[i - 1] if i - 1 < len(kappa_arr) else np.nan\n",
    "        sig_kappa = kappa_regime_signal(k, kappa_bench) if not np.isnan(k) else 0.0\n",
    "\n",
    "        # Hill thinning (compare to previous step)\n",
    "        h = hill_arr[i - 1] if i - 1 < len(hill_arr) else np.nan\n",
    "        h_prev = hill_arr[i - 2] if i - 2 >= 0 and i - 2 < len(hill_arr) else np.nan\n",
    "        sig_hill = hill_thinning_signal(h, h_prev)\n",
    "\n",
    "        # Store component signals\n",
    "        comp_lppls[i] = sig_lppls\n",
    "        comp_tc[i] = sig_tc\n",
    "        comp_var[i] = sig_var\n",
    "        comp_kappa[i] = sig_kappa\n",
    "        comp_hill[i] = sig_hill\n",
    "\n",
    "        # Aggregate\n",
    "        signal = aggregate_signals({\n",
    "            \"lppls_confidence\": sig_lppls,\n",
    "            \"lppls_tc_proximity\": sig_tc,\n",
    "            \"gpd_var_exceedance\": sig_var,\n",
    "            \"kappa_regime\": sig_kappa,\n",
    "            \"hill_thinning\": sig_hill,\n",
    "        })\n",
    "        composite[i] = signal.probability\n",
    "\n",
    "    return {\n",
    "        \"dates\": dates,\n",
    "        \"prices\": prices,\n",
    "        \"composite\": composite,\n",
    "        \"components\": {\n",
    "            \"LPPLS Confidence\": comp_lppls,\n",
    "            \"TC Proximity\": comp_tc,\n",
    "            \"VaR Exceedance\": comp_var,\n",
    "            \"Kappa Regime\": comp_kappa,\n",
    "            \"Hill Thinning\": comp_hill,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Run backtest on each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for ep in crash_episodes:\n",
    "    name = ep[\"name\"]\n",
    "    print(f\"\\nProcessing: {name}...\")\n",
    "    try:\n",
    "        df = ep[\"data\"].loc[ep[\"data_start\"]:ep[\"plot_end\"]].copy()\n",
    "\n",
    "        if len(df) < 600:\n",
    "            print(f\"  WARNING: Only {len(df)} observations. Results may be unreliable.\")\n",
    "\n",
    "        result = run_pipeline(df)\n",
    "        result[\"episode\"] = ep\n",
    "        results[name] = result\n",
    "\n",
    "        # Evaluate: signal in the 30 days before the peak\n",
    "        peak = pd.Timestamp(ep[\"peak_date\"])\n",
    "        pre_mask = (\n",
    "            (result[\"dates\"] >= peak - timedelta(days=30))\n",
    "            & (result[\"dates\"] <= peak)\n",
    "        )\n",
    "        pre_signal = result[\"composite\"][pre_mask]\n",
    "        pre_clean = pre_signal[~np.isnan(pre_signal)]\n",
    "\n",
    "        if len(pre_clean) > 0:\n",
    "            print(f\"  Signal 30d before peak: \"\n",
    "                  f\"mean={np.mean(pre_clean):.3f}, max={np.max(pre_clean):.3f}\")\n",
    "        else:\n",
    "            print(f\"  No signal data for pre-crash window.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Visualize all episodes\n",
    "\n",
    "Side-by-side: price (left) and composite crash signal (right) for each\n",
    "episode. The red shaded area marks the 30 days before the known peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = len(results)\n",
    "if n_episodes == 0:\n",
    "    print(\"No episodes completed. Check errors above.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(n_episodes, 2, figsize=(16, 4 * n_episodes))\n",
    "    if n_episodes == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (name, res) in enumerate(results.items()):\n",
    "        ep = res[\"episode\"]\n",
    "        peak = pd.Timestamp(ep[\"peak_date\"])\n",
    "\n",
    "        # Left: Price\n",
    "        ax_price = axes[i, 0]\n",
    "        ax_price.plot(res[\"dates\"], res[\"prices\"], color=\"steelblue\", linewidth=0.8)\n",
    "        ax_price.axvline(peak, color=\"red\", linestyle=\"--\", alpha=0.7,\n",
    "                         label=f\"Peak {ep['peak_date']}\")\n",
    "        ax_price.axvspan(peak - timedelta(days=30), peak, color=\"red\", alpha=0.1)\n",
    "        ax_price.set_ylabel(\"Price\")\n",
    "        ax_price.set_title(f\"{name} - Price\")\n",
    "        ax_price.legend(fontsize=8)\n",
    "\n",
    "        # Right: Composite signal\n",
    "        ax_sig = axes[i, 1]\n",
    "        ax_sig.fill_between(res[\"dates\"], 0, res[\"composite\"],\n",
    "                            color=\"orange\", alpha=0.5)\n",
    "        ax_sig.axvline(peak, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "        ax_sig.axvspan(peak - timedelta(days=30), peak, color=\"red\", alpha=0.1)\n",
    "        ax_sig.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "        ax_sig.set_ylabel(\"Crash Signal\")\n",
    "        ax_sig.set_title(f\"{name} - Composite Signal\")\n",
    "        ax_sig.set_ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Detailed component breakdown\n",
    "\n",
    "For each crash episode, we show the price alongside each individual signal\n",
    "component. This helps identify which indicators contributed to the composite signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_colors = {\n",
    "    \"LPPLS Confidence\": \"red\",\n",
    "    \"TC Proximity\": \"purple\",\n",
    "    \"VaR Exceedance\": \"orange\",\n",
    "    \"Kappa Regime\": \"green\",\n",
    "    \"Hill Thinning\": \"blue\",\n",
    "}\n",
    "\n",
    "for name, res in results.items():\n",
    "    ep = res[\"episode\"]\n",
    "    peak = pd.Timestamp(ep[\"peak_date\"])\n",
    "    components = res[\"components\"]\n",
    "    n_comp = len(components)\n",
    "\n",
    "    fig, axes = plt.subplots(n_comp + 1, 1,\n",
    "                             figsize=(14, 2.5 * (n_comp + 1)), sharex=True)\n",
    "    fig.suptitle(f\"{name}: Signal Component Breakdown\", fontsize=14, y=1.01)\n",
    "\n",
    "    # Price\n",
    "    axes[0].plot(res[\"dates\"], res[\"prices\"], color=\"steelblue\", linewidth=0.8)\n",
    "    axes[0].axvline(peak, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "    axes[0].set_ylabel(\"Price\")\n",
    "\n",
    "    # Components\n",
    "    for j, (comp_name, comp_values) in enumerate(components.items()):\n",
    "        ax = axes[j + 1]\n",
    "        color = comp_colors.get(comp_name, \"gray\")\n",
    "        ax.fill_between(res[\"dates\"], 0, comp_values, color=color, alpha=0.4)\n",
    "        ax.axvline(peak, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "        ax.set_ylabel(comp_name, fontsize=9)\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Summary scorecard\n",
    "\n",
    "For each episode and look-back window (60d, 30d, 14d before peak),\n",
    "compute the mean and max signal, plus the fraction of days above thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard = []\n",
    "\n",
    "for name, res in results.items():\n",
    "    ep = res[\"episode\"]\n",
    "    peak = pd.Timestamp(ep[\"peak_date\"])\n",
    "\n",
    "    for window_days in [60, 30, 14]:\n",
    "        pre_mask = (\n",
    "            (res[\"dates\"] >= peak - timedelta(days=window_days))\n",
    "            & (res[\"dates\"] <= peak)\n",
    "        )\n",
    "        sig = res[\"composite\"][pre_mask]\n",
    "        sig_clean = sig[~np.isnan(sig)]\n",
    "\n",
    "        if len(sig_clean) > 0:\n",
    "            scorecard.append({\n",
    "                \"episode\": name,\n",
    "                \"window\": f\"{window_days}d before peak\",\n",
    "                \"mean_signal\": np.mean(sig_clean),\n",
    "                \"max_signal\": np.max(sig_clean),\n",
    "                \"pct_above_0.3\": (sig_clean > 0.3).mean(),\n",
    "                \"pct_above_0.5\": (sig_clean > 0.5).mean(),\n",
    "            })\n",
    "\n",
    "scorecard_df = pd.DataFrame(scorecard)\n",
    "scorecard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot: max signal by episode and window\n",
    "if len(scorecard_df) > 0:\n",
    "    pivot = scorecard_df.pivot_table(\n",
    "        index=\"episode\", columns=\"window\", values=\"max_signal\",\n",
    "    )\n",
    "    print(\"Maximum composite signal before each crash:\")\n",
    "    print()\n",
    "    display(pivot.style.format(\"{:.3f}\").background_gradient(\n",
    "        cmap=\"RdYlGn_r\", vmin=0, vmax=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. False positive analysis\n",
    "\n",
    "How often does the signal exceed thresholds during non-crash periods?\n",
    "We use the full BTC series and compare crash zones vs. calm periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on full BTC history for false positive analysis\n",
    "print(\"Running full BTC pipeline for false positive analysis...\")\n",
    "full_btc = run_pipeline(data_btc)\n",
    "\n",
    "composite = full_btc[\"composite\"]\n",
    "dates = full_btc[\"dates\"]\n",
    "\n",
    "# Define crash zones: +/- 60 days around known BTC peaks\n",
    "btc_peaks = pd.to_datetime([\"2017-12-17\", \"2021-04-14\", \"2021-11-10\"])\n",
    "crash_zone = np.zeros(len(dates), dtype=bool)\n",
    "for peak in btc_peaks:\n",
    "    mask = (dates >= peak - timedelta(days=60)) & (dates <= peak + timedelta(days=30))\n",
    "    crash_zone |= mask\n",
    "\n",
    "non_crash = composite[~crash_zone & ~np.isnan(composite)]\n",
    "in_crash = composite[crash_zone & ~np.isnan(composite)]\n",
    "\n",
    "print(f\"\\nSignal statistics:\")\n",
    "print(f\"  Non-crash periods ({len(non_crash)} days):\")\n",
    "print(f\"    Mean:     {np.mean(non_crash):.4f}\")\n",
    "print(f\"    > 0.3:    {(non_crash > 0.3).mean()*100:.1f}% of days\")\n",
    "print(f\"    > 0.5:    {(non_crash > 0.5).mean()*100:.1f}% of days\")\n",
    "if len(in_crash) > 0:\n",
    "    print(f\"  Crash periods ({len(in_crash)} days):\")\n",
    "    print(f\"    Mean:     {np.mean(in_crash):.4f}\")\n",
    "    print(f\"    > 0.3:    {(in_crash > 0.3).mean()*100:.1f}% of days\")\n",
    "    print(f\"    > 0.5:    {(in_crash > 0.5).mean()*100:.1f}% of days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Cross-asset comparison\n",
    "\n",
    "Compare the signal trajectory across BTC, SPY, and Gold episodes,\n",
    "aligned to days-relative-to-peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_groups = {\n",
    "    \"BTC\": [n for n in results if \"BTC\" in n],\n",
    "    \"SPY\": [n for n in results if \"SPY\" in n or \"COVID\" in n],\n",
    "    \"Gold\": [n for n in results if \"Gold\" in n],\n",
    "}\n",
    "\n",
    "n_groups = sum(1 for v in asset_groups.values() if v)\n",
    "fig, axes = plt.subplots(1, n_groups, figsize=(6 * n_groups, 5))\n",
    "if n_groups == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "ax_idx = 0\n",
    "for asset, episode_names in asset_groups.items():\n",
    "    if not episode_names:\n",
    "        continue\n",
    "    ax = axes[ax_idx]\n",
    "    ax_idx += 1\n",
    "\n",
    "    for ep_name in episode_names:\n",
    "        if ep_name not in results:\n",
    "            continue\n",
    "        res = results[ep_name]\n",
    "        peak = pd.Timestamp(res[\"episode\"][\"peak_date\"])\n",
    "        days_to_peak = (res[\"dates\"] - peak).days\n",
    "        mask = (days_to_peak >= -365) & (days_to_peak <= 90)\n",
    "        ax.plot(days_to_peak[mask], res[\"composite\"][mask],\n",
    "                linewidth=0.8, alpha=0.8, label=ep_name)\n",
    "\n",
    "    ax.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Peak\")\n",
    "    ax.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.set_xlabel(\"Days Relative to Peak\")\n",
    "    ax.set_ylabel(\"Composite Signal\")\n",
    "    ax.set_title(asset)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key findings\n",
    "- The composite crash signal typically **rises in the 30-60 days before major peaks**.\n",
    "- Crypto bubbles tend to produce the strongest LPPLS signals due to clear\n",
    "  super-exponential growth.\n",
    "- The 2008 SPX and 2011 Gold episodes test the system on traditional assets\n",
    "  where bubble dynamics may be less pronounced.\n",
    "- The COVID crash (2020) is an exogenous shock -- LPPLS performs poorly here\n",
    "  since there was no bubble preceding the crash.\n",
    "\n",
    "### Limitations\n",
    "- This is an **in-sample** evaluation. True out-of-sample testing requires\n",
    "  live forward deployment.\n",
    "- The pipeline uses 500-day rolling windows, so the first ~2 years lack full context.\n",
    "- Different assets may benefit from different weight configurations.\n",
    "\n",
    "### Next steps\n",
    "- Use `calibrate_weights` (notebook 06) on a subset of episodes, then test on\n",
    "  held-out episodes for genuine out-of-sample evaluation.\n",
    "- Incorporate Deep LPPLS (notebook 07) and DTCAI (notebook 10) into the pipeline.\n",
    "\n",
    "*All numbers are in-sample on historical data. This is not financial advice.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}